{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(labels, convolutional_layers=2, pooling_layers=True, input_shape=None):\n",
    "    \"\"\"\n",
    "    Creates a simple convolutional network by adding layers based on input\n",
    "    \n",
    "    Parameters:\n",
    "        labels: int\n",
    "            Number of layers for prediction (i.e. size of output layer of the model). Should be positive and higher than 1.\n",
    "        convolutional_layers: int (default 2)\n",
    "            Number of convolutional layers (using tensorflows Conv2D, including the input convolutional layer). Should be positive.\n",
    "        pooling_layers: bool (default True)\n",
    "            Whether to include pooling layers between every pair of convolutional layers\n",
    "        input_shape: (int, int, int) or (int, int) or None (default None):\n",
    "            Shape of the image that will be given as input (i.e. input shape of the first layer).\n",
    "            Integers in the tuple are required to be positive.\n",
    "\n",
    "    Returns:\n",
    "        model: keras.engine.sequential.Sequential\n",
    "            Model created according to input\n",
    "    \"\"\"\n",
    "    # Input management\n",
    "    if not isinstance(labels, int):\n",
    "        raise ValueError(\"Different datatype than integer has been given as input for the number of labels\")\n",
    "    \n",
    "    if not isinstance(convolutional_layers, int):\n",
    "        raise ValueError(\"Different datatype than integer has been given as input for the number of convolutional layers\")\n",
    "\n",
    "    if labels < 1:\n",
    "        raise ValueError(\"Number of labels is less than 1. Please specify different amount.\")\n",
    "\n",
    "    if labels == 1:\n",
    "        wrn = \"\\nYou have entered 1 as the number of labels.\\n\" \n",
    "        wrn += \"This might result in unpredicted behaviour and there is not much point in building a model then\"\n",
    "        warnings.warn(wrn)\n",
    "\n",
    "    if convolutional_layers < 1:\n",
    "        raise ValueError(\"This function expects at least one convolutional layer to be present in the model.\")\n",
    "\n",
    "    if not isinstance(pooling_layers, bool):\n",
    "        raise ValueError(\"Different datatype than boolean has been given as input for the pooling_layers parameter\")\n",
    "\n",
    "    if input_shape:\n",
    "        if not isinstance(input_shape, tuple):\n",
    "            raise ValueError(\"Input shape has been assigned and different input than tuple was given\")\n",
    "        if len(input_shape) not in [2, 3]:\n",
    "            raise ValueError(\"2D or 3D images expected as input\")\n",
    "        elif len(input_shape) == 2:\n",
    "            input_shape = tuple([*input_shape, 1])\n",
    "        for val in input_shape:\n",
    "            if not isinstance(val, int):\n",
    "                raise ValueError(\"Integers were expected in place of image dimensions in parameter input_shape\")\n",
    "            if val < 0:\n",
    "                raise ValueError(\"One of the dimensions of the input shape given is negative. Please give correct input shape.\")\n",
    "\n",
    "    model = models.Sequential()\n",
    "\n",
    "    if input_shape:\n",
    "        model.add(layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=input_shape))\n",
    "    else:\n",
    "        model.add(layers.Conv2D(32, (3, 3), activation=\"relu\"))\n",
    "    for _ in range(convolutional_layers - 1):\n",
    "        if pooling_layers:\n",
    "            model.add(layers.MaxPooling2D((2, 2)))\n",
    "        model.add(layers.Conv2D(64, (3, 3), activation=\"relu\"))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation=\"relu\"))\n",
    "    model.add(layers.Dense(labels))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_dir = os.path.dirname(\"Image Collection.ipynb\")\n",
    "data_dir = os.path.join(script_dir, \"Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gestures = [\"I\", \"My\", \"You\", \"Your\", \n",
    "            \"In\", \"To\", \"With\", \"Yes\", \n",
    "            \"No\", \"Well\", \"I love you\",\n",
    "            \"Oh I see\", \"Name\", \"Hug\",\n",
    "            \"Internet\", \"Bus\", \"Money\",\n",
    "            \"Work\", \"Ask\", \"Go\",\n",
    "            \"Look\", \"Have\", \"Correct\",\n",
    "            \"Want\", \"Where\", \n",
    "            \"A\", \"B\", \"C\", \"D\", \n",
    "            \"E\", \"F\", \"G\", \"H\", \n",
    "            \"I\", \"K\", \"L\", \"M\", \n",
    "            \"N\", \"O\", \"P\", \"Q\", \n",
    "            \"R\", \"S\", \"T\", \"U\", \n",
    "            \"V\", \"W\", \"X\", \"Y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = tf.keras.preprocessing.image_dataset_from_directory(data_dir,\n",
    "                                                                   validation_split=0.25,\n",
    "                                                                  subset=\"training\",\n",
    "                                                                  seed=123,\n",
    "                                                                  image_size=(128, 128),\n",
    "                                                                  color_mode=\"grayscale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = tf.keras.preprocessing.image_dataset_from_directory(data_dir,\n",
    "                                                                   validation_split=0.25,\n",
    "                                                                  subset=\"validation\",\n",
    "                                                                  seed=123,\n",
    "                                                                  image_size=(128, 128),\n",
    "                                                                  color_mode=\"grayscale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(labels=49, convolutional_layers=2, input_shape=(128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\",\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(train_images, batch_size=200, epochs=11, \n",
    "                    validation_data=(test_images), steps_per_epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "im = cv2.imread(\"Data/Yes/Yes_88.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "im = np.expand_dims(im, axis=0)\n",
    "im = np.expand_dims(im, axis=-1)\n",
    "prediction = model(im, training=False)\n",
    "os.listdir(\"Data\")[np.argmax(prediction, axis = 1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"Weights/weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = cv2.imread(\"Data/Yes/Yes_88.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "im = np.expand_dims(im, axis=0)\n",
    "im = np.expand_dims(im, axis=-1)\n",
    "prediction = model(im, training=False)\n",
    "os.listdir(\"Data\")[np.argmax(prediction, axis = 1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = fig.get_figure()    \n",
    "figure.savefig(\"training.png\", dpi=400) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(history.history)\n",
    "df.drop(columns=[\"val_loss\", \"loss\"], inplace=True)\n",
    "\n",
    "plt.figure(figsize=(15, 10), facecolor=(1, 1, 1))\n",
    "\n",
    "fig = sb.lineplot(data=df, sizes=[1.7, 1.7])\n",
    "plt.title(\"Progression of model performance with each epoch\", fontsize=20)\n",
    "plt.xlabel(\"Number of epochs\", fontsize=16)\n",
    "plt.xticks(range(0, 11), fontsize=14)\n",
    "plt.ylabel(\"Accuracy\", fontsize=16)\n",
    "plt.yticks(np.arange(0, 1.1, 0.1), fontsize=14)\n",
    "plt.legend([\"Training accuracy\", \"Validation accuracy\"], fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"Weights/weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#import cv2\n",
    "\n",
    "#im = cv2.imread(\"Data/Yes/Yes_2.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "#im = np.expand_dims(im, axis=0)\n",
    "#im = np.expand_dims(im, axis=-1)\n",
    "#prediction = model(im, training=False)\n",
    "#test_images.class_names[np.argmax(prediction, axis = 1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Building data pipeline</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set threading options to autotuning\n",
    "tf.config.threading.set_inter_op_parallelism_threads(0)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.json\", \"r\") as file:\n",
    "    config = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Paths': {'Data pipelines': 'data_pipelines',\n",
       "  'Logs': 'logs',\n",
       "  'Translations': 'translations.txt',\n",
       "  'Gesture list': 'gestures.txt'},\n",
       " 'Model': {'Current model': 'model/current',\n",
       "  'Model experiments': 'model/experiments'},\n",
       " 'General parameters': {'Image size': 196,\n",
       "  'Desired amount': 100,\n",
       "  'Top-up amount': 100}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Specify validation split size in config\n",
    "# TODO: Specify batch size in config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that image_dataset_from_directory shuffles dataset by default during loading (can be turned off)\n",
    "    # Source: https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 490 files belonging to 49 classes.\n",
      "Using 368 files for training.\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(config[\"Paths\"][\"Data\"],\n",
    "                                                       validation_split=0.25,\n",
    "                                                       subset=\"training\",\n",
    "                                                       seed=123,\n",
    "                                                       label_mode=\"categorical\",\n",
    "                                                       image_size=(config[\"General parameters\"][\"Image size\"],\n",
    "                                                                   config[\"General parameters\"][\"Image size\"]),\n",
    "                                                       batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = tf.keras.utils.image_dataset_from_directory(config[\"Paths\"][\"Data\"],\n",
    "                                                      validation_split=0.25,\n",
    "                                                      subset=\"train\",\n",
    "                                                      seed=123,\n",
    "                                                      label_mode=\"categorical\",\n",
    "                                                      image_size=(config[\"General parameters\"][\"Image size\"],\n",
    "                                                                  config[\"General parameters\"][\"Image size\"]),\n",
    "                                                      batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config[\"Paths\"][\"Gesture list\"], \"r\") as gesture_list:\n",
    "    gestures = gesture_list.readlines()[0].split(\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I index', 'My', 'You', 'Your', 'In', 'To', 'With', 'Yes', 'No', 'Well', 'I love you', 'Oh I see', 'Name', 'Hug', 'Internet', 'Bus', 'Money', 'Work', 'Ask', 'Go', 'Look', 'Have', 'Correct', 'Want', 'Where', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y']\n"
     ]
    }
   ],
   "source": [
    "print(gestures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 490 files belonging to 49 classes.\n",
      "Using 392 files for training.\n",
      "Using 98 files for validation.\n"
     ]
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_images, test_images = tf.keras.preprocessing.image_dataset_from_directory(\"Data\",\n",
    "                                                                                labels=\"inferred\",\n",
    "                                                                                label_mode=\"categorical\",\n",
    "                                                                                class_names=gestures,\n",
    "                                                                                batch_size=32,\n",
    "                                                                                image_size=(config[\"General parameters\"][\"Image size\"],\n",
    "                                                                                            config[\"General parameters\"][\"Image size\"]),\n",
    "                                                                                shuffle=True,\n",
    "                                                                                seed=123,\n",
    "                                                                                validation_split=0.2,\n",
    "                                                                                subset=\"both\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_images = test_images.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Sequential\n",
    "from model.preprocessing import AdaptiveThresholding, Blurring, Grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline1 = Sequential(\n",
    "            [\n",
    "                Grayscale(),\n",
    "                Blurring(blurring_type=\"median\", kernel_size=3, sigma=None),\n",
    "                AdaptiveThresholding(thresholding_type=\"mean\", block_size=3, constant=0),\n",
    "                layers.Rescaling(scale=(1. / 255))\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    inputs = tf.keras.Input(shape=(196, 196, 3))\n",
    "    x = Grayscale()(inputs)\n",
    "    x = Blurring()(x)\n",
    "    x = AdaptiveThresholding()(x)\n",
    "    x = tf.keras.layers.Rescaling(scale=1./255)(x)\n",
    "    x = tf.keras.layers.Conv2D(16, 3, activation='relu')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D()(x)\n",
    "    x = tf.keras.layers.Conv2D(32, 3, activation='relu')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D()(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "    outputs = tf.keras.layers.Dense(49, activation='softmax')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7184\\2963407278.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfull_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7184\\228992991.py\u001b[0m in \u001b[0;36mcreate_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#x = AdaptiveThresholding()(x)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m#x = tf.keras.layers.Rescaling(scale=1./255)(x)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\asl\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\asl\\lib\\site-packages\\keras\\layers\\convolutional\\base_conv.py\u001b[0m in \u001b[0;36m_get_input_channel\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    406\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_input_channel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m         \u001b[0mchannel_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_channel_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 408\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdims\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchannel_axis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    409\u001b[0m             raise ValueError(\n\u001b[0;32m    410\u001b[0m                 \u001b[1;34m\"The channel dimension of the inputs should be defined. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "full_model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model.compile(optimizer=\"adam\",\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_15 (InputLayer)       [(None, 196, 196, 3)]     0         \n",
      "                                                                 \n",
      " grayscale_17 (Grayscale)    (None, 196, 196, 1)       0         \n",
      "                                                                 \n",
      " rescaling_15 (Rescaling)    (None, 196, 196, 1)       0         \n",
      "                                                                 \n",
      " lambda_1 (Lambda)           (None, 196, 196, 1)       0         \n",
      "                                                                 \n",
      " conv2d_20 (Conv2D)          (None, 194, 194, 16)      160       \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 97, 97, 16)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_21 (Conv2D)          (None, 95, 95, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPoolin  (None, 47, 47, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 70688)             0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 64)                4524096   \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 49)                3185      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,532,081\n",
      "Trainable params: 4,532,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "full_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 5s 340ms/step - loss: 4.2616 - accuracy: 0.0230 - val_loss: 3.8473 - val_accuracy: 0.0408\n"
     ]
    }
   ],
   "source": [
    "history = full_model.fit(train_images, epochs=1, \n",
    "                    validation_data=(test_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ds = tf.data.Dataset.list_files(str(\"Old_data/data_64x64/*/*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in list_ds.take(5):\n",
    "    print(f.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load images and extract labels\n",
    "def process_paths(dir_name):\n",
    "    label = tf.strings.split(dir_name, os.sep)[-2]\n",
    "    return tf.io.read_file(dir_name), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_ds = list_ds.map(process_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_raw, label_text in labeled_ds.take(1):\n",
    "    print(repr(image_raw.numpy()[:10]))\n",
    "    print()\n",
    "    print(label_text.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While large buffer_sizes shuffle more thoroughly,\n",
    "# they can take a lot of memory, and significant time to fill.\n",
    "# (quote: tensorflow.org/guide/data)\n",
    "labeled_ds = labeled_ds.shuffle(buffer_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch dataset and get full element shape propagation using drop_remainder=True\n",
    "batched_dataset = labeled_ds.batch(100, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in batched_dataset.take(1):\n",
    "    print([sample.numpy()[:1] for sample in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
