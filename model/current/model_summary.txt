Preprocessing pipeline:
Model: "preprocessing_pipeline"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 preprocessing_input (InputL  [(None, 196, 196, 3)]    0         
 ayer)                                                           
                                                                 
 grayscale (Grayscale)       (None, 196, 196, 1)       0         
                                                                 
 adaptive_thresholding (Adap  (None, 196, 196, 1)      0         
 tiveThresholding)                                               
                                                                 
 rescaling (Rescaling)       (None, 196, 196, 1)       0         
                                                                 
=================================================================
Total params: 0
Trainable params: 0
Non-trainable params: 0
_________________________________________________________________


Trainable summary:
Model: "trainable_layers"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 trainable_input (InputLayer  [(None, 196, 196, 1)]    0         
 )                                                               
                                                                 
 conv2d (Conv2D)             (None, 192, 192, 128)     3328      
                                                                 
 max_pooling2d (MaxPooling2D  (None, 48, 48, 128)      0         
 )                                                               
                                                                 
 conv2d_1 (Conv2D)           (None, 44, 44, 256)       819456    
                                                                 
 max_pooling2d_1 (MaxPooling  (None, 11, 11, 256)      0         
 2D)                                                             
                                                                 
 flatten (Flatten)           (None, 30976)             0         
                                                                 
 dense (Dense)               (None, 256)               7929856   
                                                                 
 batch_normalization (BatchN  (None, 256)              1024      
 ormalization)                                                   
                                                                 
 re_lu (ReLU)                (None, 256)               0         
                                                                 
 dense_1 (Dense)             (None, 512)               131584    
                                                                 
 output_dense (Dense)        (None, 49)                25137     
                                                                 
 softmax_output (Activation)  (None, 49)               0         
                                                                 
=================================================================
Total params: 8,910,385
Trainable params: 8,909,873
Non-trainable params: 512
_________________________________________________________________



Training parameters: {'verbose': 1, 'epochs': 100, 'steps': 128}
Final training accuracy: 0.9675031900405884
Final validation accuracy: 0.9905616641044617
Final training precision: 0.9811862111091614
Final validation precision: 0.9925025105476379
Final training recall: 0.9588619470596313
Final validation recall: 0.9891316294670105
Final training f1_score: 
0.9674878120422363
Final validation f1_score: 
0.9906147718429565


Command line arguments: 
config_dir: 
collect: False
preprocess: False
train: True
showcase: False
predict: False
guided: False
experiment: 72
tensorboard: True
early_stopping: disable
disable_checkpoint: True
seed: 123
split: 0.3
augmentation: False
randaugment: 10,3
batch_size: 512
epochs: 100
optimizer: SGD
learning_rate: 0.04
lr_decay: True
lr_warmup: False
momentum: 0.95
weight_decay: 0.0003
label_smoothing: 0
architecture: I,C-k5-f128-s1,P-tm-p3-s4,C-k5-f256-s1,P-tm-p3-s4,H-256,B,H-512,O
preprocessing_layers: I,G,T-tm-b7-c(3),R
