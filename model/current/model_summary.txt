Preprocessing pipeline:
Model: "preprocessing_pipeline"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 preprocessing_input (InputL  [(None, 196, 196, 3)]    0         
 ayer)                                                           
                                                                 
 grayscale (Grayscale)       (None, 196, 196, 1)       0         
                                                                 
 adaptive_thresholding (Adap  (None, 196, 196, 1)      0         
 tiveThresholding)                                               
                                                                 
 rescaling (Rescaling)       (None, 196, 196, 1)       0         
                                                                 
=================================================================
Total params: 0
Trainable params: 0
Non-trainable params: 0
_________________________________________________________________


Trainable summary:
Model: "trainable_layers"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 trainable_input (InputLayer  [(None, 196, 196, 1)]    0         
 )                                                               
                                                                 
 conv2d (Conv2D)             (None, 65, 65, 64)        640       
                                                                 
 max_pooling2d (MaxPooling2D  (None, 32, 32, 64)       0         
 )                                                               
                                                                 
 conv2d_1 (Conv2D)           (None, 10, 10, 64)        36928     
                                                                 
 max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         
 2D)                                                             
                                                                 
 batch_normalization (BatchN  (None, 5, 5, 64)         256       
 ormalization)                                                   
                                                                 
 re_lu (ReLU)                (None, 5, 5, 64)          0         
                                                                 
 flatten (Flatten)           (None, 1600)              0         
                                                                 
 dense (Dense)               (None, 512)               819712    
                                                                 
 softmax_output (Dense)      (None, 49)                25137     
                                                                 
=================================================================
Total params: 882,673
Trainable params: 882,545
Non-trainable params: 128
_________________________________________________________________



Training parameters: {'verbose': 1, 'epochs': 29, 'steps': 255}
Final training accuracy: 0.9050070643424988
Final validation accuracy: 0.9256730079650879
Final training precision: 0.9838435649871826
Final validation precision: 0.9820179343223572
Final training recall: 0.7809186577796936
Final validation recall: 0.8180615901947021
Final training f1_score: 
0.9050899147987366
Final validation f1_score: 
0.9262667298316956


Command line arguments: 
config_dir: 
collect: False
preprocess: False
train: True
showcase: False
predict: False
guided: False
experiment: 10
tensorboard: True
early_stopping: disable
disable_checkpoint: False
augmentation: False
randaugment: 5,2
batch_size: 256
epochs: 29
optimizer: SGD
learning_rate: 0.01
lr_decay: 0.85
lr_decay_iterations: 10000
momentum: 0.95
weight_decay: 0.0001
label_smoothing: 0.1
seed: 123
split: 0.3
architecture: I,C-f64-k3-s3,P-tm-p2-s2,C-f64-k3-s3,P-tm-p2-s2,B,F,H-512,O
preprocessing_layers: I,G,T-tm-b7-c(3),R
