{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(labels, convolutional_layers=2, pooling_layers=True, input_shape=None):\n",
    "    \"\"\"\n",
    "    Creates a simple convolutional network by adding layers based on input\n",
    "    \n",
    "    Parameters:\n",
    "        labels: int\n",
    "            Number of layers for prediction (i.e. size of output layer of the model). Should be positive and higher than 1.\n",
    "        convolutional_layers: int (default 2)\n",
    "            Number of convolutional layers (using tensorflows Conv2D, including the input convolutional layer). Should be positive.\n",
    "        pooling_layers: bool (default True)\n",
    "            Whether to include pooling layers between every pair of convolutional layers\n",
    "        input_shape: (int, int, int) or (int, int) or None (default None):\n",
    "            Shape of the image that will be given as input (i.e. input shape of the first layer).\n",
    "            Integers in the tuple are required to be positive.\n",
    "\n",
    "    Returns:\n",
    "        model: keras.engine.sequential.Sequential\n",
    "            Model created according to input\n",
    "    \"\"\"\n",
    "    # Input management\n",
    "    if not isinstance(labels, int):\n",
    "        raise ValueError(\"Different datatype than integer has been given as input for the number of labels\")\n",
    "    \n",
    "    if not isinstance(convolutional_layers, int):\n",
    "        raise ValueError(\"Different datatype than integer has been given as input for the number of convolutional layers\")\n",
    "\n",
    "    if labels < 1:\n",
    "        raise ValueError(\"Number of labels is less than 1. Please specify different amount.\")\n",
    "\n",
    "    if labels == 1:\n",
    "        wrn = \"\\nYou have entered 1 as the number of labels.\\n\" \n",
    "        wrn += \"This might result in unpredicted behaviour and there is not much point in building a model then\"\n",
    "        warnings.warn(wrn)\n",
    "\n",
    "    if convolutional_layers < 1:\n",
    "        raise ValueError(\"This function expects at least one convolutional layer to be present in the model.\")\n",
    "\n",
    "    if not isinstance(pooling_layers, bool):\n",
    "        raise ValueError(\"Different datatype than boolean has been given as input for the pooling_layers parameter\")\n",
    "\n",
    "    if input_shape:\n",
    "        if not isinstance(input_shape, tuple):\n",
    "            raise ValueError(\"Input shape has been assigned and different input than tuple was given\")\n",
    "        if len(input_shape) not in [2, 3]:\n",
    "            raise ValueError(\"2D or 3D images expected as input\")\n",
    "        elif len(input_shape) == 2:\n",
    "            input_shape = tuple([*input_shape, 1])\n",
    "        for val in input_shape:\n",
    "            if not isinstance(val, int):\n",
    "                raise ValueError(\"Integers were expected in place of image dimensions in parameter input_shape\")\n",
    "            if val < 0:\n",
    "                raise ValueError(\"One of the dimensions of the input shape given is negative. Please give correct input shape.\")\n",
    "\n",
    "    model = models.Sequential()\n",
    "\n",
    "    if input_shape:\n",
    "        model.add(layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=input_shape))\n",
    "    else:\n",
    "        model.add(layers.Conv2D(32, (3, 3), activation=\"relu\"))\n",
    "    for _ in range(convolutional_layers - 1):\n",
    "        if pooling_layers:\n",
    "            model.add(layers.MaxPooling2D((2, 2)))\n",
    "        model.add(layers.Conv2D(64, (3, 3), activation=\"relu\"))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation=\"relu\"))\n",
    "    model.add(layers.Dense(labels))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_dir = os.path.dirname(\"Image Collection.ipynb\")\n",
    "data_dir = os.path.join(script_dir, \"Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gestures = [\"I\", \"My\", \"You\", \"Your\", \n",
    "            \"In\", \"To\", \"With\", \"Yes\", \n",
    "            \"No\", \"Well\", \"I love you\",\n",
    "            \"Oh I see\", \"Name\", \"Hug\",\n",
    "            \"Internet\", \"Bus\", \"Money\",\n",
    "            \"Work\", \"Ask\", \"Go\",\n",
    "            \"Look\", \"Have\", \"Correct\",\n",
    "            \"Want\", \"Where\", \n",
    "            \"A\", \"B\", \"C\", \"D\", \n",
    "            \"E\", \"F\", \"G\", \"H\", \n",
    "            \"I\", \"K\", \"L\", \"M\", \n",
    "            \"N\", \"O\", \"P\", \"Q\", \n",
    "            \"R\", \"S\", \"T\", \"U\", \n",
    "            \"V\", \"W\", \"X\", \"Y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = tf.keras.preprocessing.image_dataset_from_directory(data_dir,\n",
    "                                                                   validation_split=0.25,\n",
    "                                                                  subset=\"training\",\n",
    "                                                                  seed=123,\n",
    "                                                                  image_size=(128, 128),\n",
    "                                                                  color_mode=\"grayscale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = tf.keras.preprocessing.image_dataset_from_directory(data_dir,\n",
    "                                                                   validation_split=0.25,\n",
    "                                                                  subset=\"validation\",\n",
    "                                                                  seed=123,\n",
    "                                                                  image_size=(128, 128),\n",
    "                                                                  color_mode=\"grayscale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(labels=49, convolutional_layers=2, input_shape=(128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\",\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(train_images, batch_size=200, epochs=11, \n",
    "                    validation_data=(test_images), steps_per_epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "im = cv2.imread(\"Data/Yes/Yes_88.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "im = np.expand_dims(im, axis=0)\n",
    "im = np.expand_dims(im, axis=-1)\n",
    "prediction = model(im, training=False)\n",
    "os.listdir(\"Data\")[np.argmax(prediction, axis = 1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"Weights/weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = cv2.imread(\"Data/Yes/Yes_88.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "im = np.expand_dims(im, axis=0)\n",
    "im = np.expand_dims(im, axis=-1)\n",
    "prediction = model(im, training=False)\n",
    "os.listdir(\"Data\")[np.argmax(prediction, axis = 1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = fig.get_figure()    \n",
    "figure.savefig(\"training.png\", dpi=400) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(history.history)\n",
    "df.drop(columns=[\"val_loss\", \"loss\"], inplace=True)\n",
    "\n",
    "plt.figure(figsize=(15, 10), facecolor=(1, 1, 1))\n",
    "\n",
    "fig = sb.lineplot(data=df, sizes=[1.7, 1.7])\n",
    "plt.title(\"Progression of model performance with each epoch\", fontsize=20)\n",
    "plt.xlabel(\"Number of epochs\", fontsize=16)\n",
    "plt.xticks(range(0, 11), fontsize=14)\n",
    "plt.ylabel(\"Accuracy\", fontsize=16)\n",
    "plt.yticks(np.arange(0, 1.1, 0.1), fontsize=14)\n",
    "plt.legend([\"Training accuracy\", \"Validation accuracy\"], fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"Weights/weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#import cv2\n",
    "\n",
    "#im = cv2.imread(\"Data/Yes/Yes_2.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "#im = np.expand_dims(im, axis=0)\n",
    "#im = np.expand_dims(im, axis=-1)\n",
    "#prediction = model(im, training=False)\n",
    "#test_images.class_names[np.argmax(prediction, axis = 1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Building data pipeline</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set threading options to autotuning\n",
    "tf.config.threading.set_inter_op_parallelism_threads(0)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.json\", \"r\") as file:\n",
    "    config = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Paths': {'Data pipelines': 'data_pipelines',\n",
       "  'Logs': 'logs',\n",
       "  'Translations': 'translations.txt',\n",
       "  'Gesture list': 'gestures.txt'},\n",
       " 'Model': {'Current model': 'model/current',\n",
       "  'Model experiments': 'model/experiments'},\n",
       " 'General parameters': {'Image size': 196,\n",
       "  'Desired amount': 100,\n",
       "  'Top-up amount': 100}}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Specify validation split size in config\n",
    "# TODO: Specify batch size in config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that image_dataset_from_directory shuffles dataset by default during loading (can be turned off)\n",
    "    # Source: https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 490 files belonging to 49 classes.\n",
      "Using 368 files for training.\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(config[\"Paths\"][\"Data\"],\n",
    "                                                       validation_split=0.25,\n",
    "                                                       subset=\"training\",\n",
    "                                                       seed=123,\n",
    "                                                       label_mode=\"categorical\",\n",
    "                                                       image_size=(config[\"General parameters\"][\"Image size\"],\n",
    "                                                                   config[\"General parameters\"][\"Image size\"]),\n",
    "                                                       batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = tf.keras.utils.image_dataset_from_directory(config[\"Paths\"][\"Data\"],\n",
    "                                                      validation_split=0.25,\n",
    "                                                      subset=\"train\",\n",
    "                                                      seed=123,\n",
    "                                                      label_mode=\"categorical\",\n",
    "                                                      image_size=(config[\"General parameters\"][\"Image size\"],\n",
    "                                                                  config[\"General parameters\"][\"Image size\"]),\n",
    "                                                      batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config[\"Paths\"][\"Gesture list\"], \"r\") as gesture_list:\n",
    "    gestures = gesture_list.readlines()[0].split(\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I index', 'My', 'You', 'Your', 'In', 'To', 'With', 'Yes', 'No', 'Well', 'I love you', 'Oh I see', 'Name', 'Hug', 'Internet', 'Bus', 'Money', 'Work', 'Ask', 'Go', 'Look', 'Have', 'Correct', 'Want', 'Where', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y']\n"
     ]
    }
   ],
   "source": [
    "print(gestures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 490 files belonging to 49 classes.\n",
      "Using 392 files for training.\n",
      "Using 98 files for validation.\n"
     ]
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_images, test_images = tf.keras.preprocessing.image_dataset_from_directory(\"Data\",\n",
    "                                                                                labels=\"inferred\",\n",
    "                                                                                label_mode=\"categorical\",\n",
    "                                                                                class_names=gestures,\n",
    "                                                                                batch_size=32,\n",
    "                                                                                image_size=(config[\"General parameters\"][\"Image size\"],\n",
    "                                                                                            config[\"General parameters\"][\"Image size\"]),\n",
    "                                                                                shuffle=True,\n",
    "                                                                                seed=123,\n",
    "                                                                                validation_split=0.2,\n",
    "                                                                                subset=\"both\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_images = test_images.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(labels=49, convolutional_layers=2, input_shape=(196, 196, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "13/13 [==============================] - 14s 909ms/step - loss: 8.2143 - accuracy: 0.0153 - val_loss: 9.7090 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "13/13 [==============================] - 10s 799ms/step - loss: 7.8750 - accuracy: 0.0153 - val_loss: 6.7433 - val_accuracy: 0.0306\n",
      "Epoch 3/5\n",
      "13/13 [==============================] - 10s 804ms/step - loss: 8.8841 - accuracy: 0.0179 - val_loss: 8.8814 - val_accuracy: 0.0306\n",
      "Epoch 4/5\n",
      "13/13 [==============================] - 11s 853ms/step - loss: 7.9770 - accuracy: 0.0332 - val_loss: 8.5525 - val_accuracy: 0.0408\n",
      "Epoch 5/5\n",
      "13/13 [==============================] - 15s 1s/step - loss: 8.3058 - accuracy: 0.0408 - val_loss: 9.3748 - val_accuracy: 0.0408\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\",\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(train_images, epochs=5, \n",
    "                    validation_data=(test_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ds = tf.data.Dataset.list_files(str(\"Old_data/data_64x64/*/*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in list_ds.take(5):\n",
    "    print(f.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load images and extract labels\n",
    "def process_paths(dir_name):\n",
    "    label = tf.strings.split(dir_name, os.sep)[-2]\n",
    "    return tf.io.read_file(dir_name), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_ds = list_ds.map(process_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_raw, label_text in labeled_ds.take(1):\n",
    "    print(repr(image_raw.numpy()[:10]))\n",
    "    print()\n",
    "    print(label_text.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While large buffer_sizes shuffle more thoroughly,\n",
    "# they can take a lot of memory, and significant time to fill.\n",
    "# (quote: tensorflow.org/guide/data)\n",
    "labeled_ds = labeled_ds.shuffle(buffer_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch dataset and get full element shape propagation using drop_remainder=True\n",
    "batched_dataset = labeled_ds.batch(100, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in batched_dataset.take(1):\n",
    "    print([sample.numpy()[:1] for sample in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You do not have pycocotools installed, so KerasCV pycoco metrics are not available. Please run `pip install pycocotools`.\n",
      "You do not have pyococotools installed, so the `PyCOCOCallback` API is not available.\n",
      "You do not have Waymo Open Dataset installed, so KerasCV Waymo metrics are not available.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, Sequential\n",
    "from keras_cv.layers import Grayscale\n",
    "from model.preprocessing import AdaptiveThresholding, Blurring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline1 = Sequential(\n",
    "            [\n",
    "                Grayscale(),\n",
    "                Blurring(blurring_type=\"median\", kernel_size=3, sigma=None),\n",
    "                AdaptiveThresholding(thresholding_type=\"mean\", block_size=3, constant=0),\n",
    "                layers.Rescaling(scale=(1. / 255))\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\tomas\\anaconda3\\envs\\asl\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(196, 196), dtype=float32, numpy=\n",
       "array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline1(tf.ones([196, 196, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " grayscale (Grayscale)       (196, 196, 1)             0         \n",
      "                                                                 \n",
      " blurring (Blurring)         None                      0         \n",
      "                                                                 \n",
      " adaptive_thresholding (Adap  None                     0         \n",
      " tiveThresholding)                                               \n",
      "                                                                 \n",
      " rescaling (Rescaling)       None                      0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pipeline1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<model.preprocessing.Blurring at 0x1686dbdcdf0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline1.layers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs=<PrefetchDataset element_spec=(TensorSpec(shape=(None, 196, 196, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 49), dtype=tf.float32, name=None))>. Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer 'grayscale' (type Grayscale).\n\nAttempt to convert a value (<PrefetchDataset element_spec=(TensorSpec(shape=(None, 196, 196, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 49), dtype=tf.float32, name=None))>) with an unsupported type (<class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset'>) to a Tensor.\n\nCall arguments received by layer 'grayscale' (type Grayscale):\n  • inputs=<PrefetchDataset element_spec=(TensorSpec(shape=(None, 196, 196, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 49), dtype=tf.float32, name=None))>\n  • training=True",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4804\\1174493325.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpipeline1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\asl\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\asl\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m   \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer 'grayscale' (type Grayscale).\n\nAttempt to convert a value (<PrefetchDataset element_spec=(TensorSpec(shape=(None, 196, 196, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 49), dtype=tf.float32, name=None))>) with an unsupported type (<class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset'>) to a Tensor.\n\nCall arguments received by layer 'grayscale' (type Grayscale):\n  • inputs=<PrefetchDataset element_spec=(TensorSpec(shape=(None, 196, 196, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 49), dtype=tf.float32, name=None))>\n  • training=True"
     ]
    }
   ],
   "source": [
    "pipeline1(train_images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
