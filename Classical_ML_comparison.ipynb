{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a71ac7b",
   "metadata": {},
   "source": [
    "<h1>Dataset Preprocessing to Enable Application of Classical ML Models</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "250cc579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e8a40f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1d153247",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.model import build_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "368d83c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration file from json in the given folder\n",
    "with open(\"config.json\", \"r\") as config_file:\n",
    "    config = json.load(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e649e989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the list of gestures\n",
    "with open(config[\"Paths\"][\"Gesture list\"], \"r\") as gesture_list:\n",
    "    gestures = gesture_list.readlines()[0].split(\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "49cc51b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = config[\"General parameters\"][\"Image size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "49925073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26351 files belonging to 49 classes.\n",
      "Using 21081 files for training.\n",
      "Using 5270 files for validation.\n"
     ]
    }
   ],
   "source": [
    "train_images, test_images = tf.keras.preprocessing.image_dataset_from_directory(\"Data\",\n",
    "                                                                                labels=\"inferred\",\n",
    "                                                                                label_mode=\"int\",\n",
    "                                                                                class_names=gestures,\n",
    "                                                                                color_mode=\"rgb\",\n",
    "                                                                                batch_size=128,\n",
    "                                                                                image_size=(img_size,\n",
    "                                                                                            img_size),\n",
    "                                                                                shuffle=True,\n",
    "                                                                                seed=42,\n",
    "                                                                                validation_split=0.2,\n",
    "                                                                                subset=\"both\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "bf1a7bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the default preprocessing pipeline if not specified\n",
    "preprocessing_layers = config[\"Model\"][\"Default preprocessing\"]\n",
    "preprocessing_layers = \"I,R\"\n",
    "\n",
    "# Build the preprocessing pipeline according to given instructions\n",
    "preprocessing = build_preprocessing(inp_shape=[img_size,\n",
    "                                               img_size,\n",
    "                                               3],\n",
    "                                    instructions=preprocessing_layers,\n",
    "                                    name=\"preprocessing_pipeline\")\n",
    "img_size = 64\n",
    "resize = tf.keras.layers.Resizing(img_size, img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f2e0ad5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the same preprocessing steps as for the CNN but flatten the images\n",
    "channels = 1 if \"G\" in preprocessing_layers else 3\n",
    "train_images = train_images.map(lambda x, y: (tf.reshape(resize(preprocessing(x)), [-1, img_size ** 2 * channels]), y)).unbatch()\n",
    "test_images = test_images.map(lambda x, y: (tf.reshape(resize(preprocessing(x)), [-1, img_size ** 2 * channels]), y)).unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c7f605f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the tf.data.Datasets to numpy arrays containing image & label per sample\n",
    "train_images = np.array(list(train_images.take(10000).as_numpy_iterator()), dtype=tuple)\n",
    "test_images = np.array(list(test_images.take(2000).as_numpy_iterator()), dtype=tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8daeada4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the images and the labels separately\n",
    "train_X, train_y = np.array([element[0] for element in train_images]), np.ravel(np.vstack([element[1] for element in train_images]))\n",
    "test_X, test_y = np.array([element[0] for element in test_images]), np.ravel(np.vstack([element[1] for element in test_images]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4206575f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset memory consumption: 468.8MB\n"
     ]
    }
   ],
   "source": [
    "# Check memory consumption for the training dataset\n",
    "print(\"Training dataset memory consumption: {:.1f}MB\".format(train_X.nbytes / (1024 * 1024.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b437dcf4",
   "metadata": {},
   "source": [
    "<h1>k-NN Gesture Classification</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "195314ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05968405",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 21 candidates, totalling 105 fits\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the k-NN classifier\n",
    "knn = KNeighborsClassifier(n_jobs=5)\n",
    "\n",
    "# Initialize the grid search\n",
    "parameters = {\"p\": [1, 2],\n",
    "              \"n_neighbors\": [1, 3, 5, 10, 15]}\n",
    "clf = GridSearchCV(knn, parameters,\n",
    "                   scoring=\"accuracy\",\n",
    "                   n_jobs=5,\n",
    "                   refit=True,\n",
    "                   verbose=3,\n",
    "                   cv=3)\n",
    "clf.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "5c91fd8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9025\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the classifier on the test dataset\n",
    "knn_predictions = knn.predict(test_X)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(test_y, knn_predictions))\n",
    "#print(classification_report(test_y,\n",
    "#                            knn_predictions,\n",
    "#                            labels=gestures[:-1],\n",
    "#                            target_names=gestures[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "48e6677f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "        34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]),\n",
       " array([42, 38, 41, 40, 33, 54, 43, 41, 51, 32, 23, 32, 46, 35, 37, 46, 43,\n",
       "        38, 37, 42, 50, 44, 41, 45, 50, 54, 38, 56, 42, 35, 46, 43, 42, 44,\n",
       "        41, 50, 48, 46, 38, 33, 39, 40, 43, 51, 36, 33, 34, 44],\n",
       "       dtype=int64))"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(knn_predictions, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "cb794d0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "        34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]),\n",
       " array([41, 40, 37, 38, 31, 54, 43, 47, 46, 44, 22, 39, 35, 32, 37, 47, 45,\n",
       "        39, 38, 45, 55, 41, 40, 45, 44, 46, 40, 46, 39, 38, 46, 36, 30, 52,\n",
       "        43, 41, 44, 47, 39, 33, 39, 46, 48, 51, 38, 45, 40, 48],\n",
       "       dtype=int64))"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(test_y, return_counts=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
