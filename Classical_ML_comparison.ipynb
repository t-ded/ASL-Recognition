{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a71ac7b",
   "metadata": {},
   "source": [
    "<h1>Dataset Preprocessing to Enable Application of Classical ML Models</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "250cc579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8a40f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d153247",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.model import build_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "368d83c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration file from json in the given folder\n",
    "with open(\"config.json\", \"r\") as config_file:\n",
    "    config = json.load(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e649e989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the list of gestures\n",
    "with open(config[\"Paths\"][\"Gesture list\"], \"r\") as gesture_list:\n",
    "    gestures = gesture_list.readlines()[0].split(\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "49cc51b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = config[\"General parameters\"][\"Image size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49925073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26351 files belonging to 49 classes.\n",
      "Using 21081 files for training.\n",
      "Using 5270 files for validation.\n"
     ]
    }
   ],
   "source": [
    "train_images, test_images = tf.keras.preprocessing.image_dataset_from_directory(\"Data\",\n",
    "                                                                                labels=\"inferred\",\n",
    "                                                                                label_mode=\"int\",\n",
    "                                                                                class_names=gestures,\n",
    "                                                                                color_mode=\"rgb\",\n",
    "                                                                                batch_size=128,\n",
    "                                                                                image_size=(img_size,\n",
    "                                                                                            img_size),\n",
    "                                                                                shuffle=True,\n",
    "                                                                                seed=42,\n",
    "                                                                                validation_split=0.2,\n",
    "                                                                                subset=\"both\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf1a7bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the default preprocessing pipeline if not specified\n",
    "preprocessing_layers = config[\"Model\"][\"Default preprocessing\"]\n",
    "#preprocessing_layers = \"I,R\"\n",
    "\n",
    "# Build the preprocessing pipeline according to given instructions\n",
    "preprocessing = build_preprocessing(inp_shape=[img_size,\n",
    "                                               img_size,\n",
    "                                               3],\n",
    "                                    instructions=preprocessing_layers,\n",
    "                                    name=\"preprocessing_pipeline\")\n",
    "img_size = 128\n",
    "resize = tf.keras.layers.Resizing(img_size, img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "992fd40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the same preprocessing steps as for the CNN but flatten the images\n",
    "train_images = train_images.map(lambda x, y: (tf.reshape(resize(preprocessing(x)), [-1, img_size ** 2]), y)).unbatch()\n",
    "test_images = test_images.map(lambda x, y: (tf.reshape(resize(preprocessing(x)), [-1, img_size ** 2]), y)).unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7f605f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the tf.data.Datasets to numpy arrays containing image & label per sample\n",
    "train_images = np.fromiter(tfds.as_numpy(train_images.take(10000)), dtype=tuple, count=10000)\n",
    "test_images = np.fromiter(tfds.as_numpy(test_images.take(2000)), dtype=tuple, count=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daeada4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the images and the labels separately\n",
    "train_X, train_y = np.vstack([element[0] for element in train_images]), np.ravel(np.vstack([element[1] for element in train_images]))\n",
    "test_X, test_y = np.vstack([element[0] for element in test_images]), np.ravel(np.vstack([element[1] for element in test_images]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4206575f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check memory consumption for the training dataset\n",
    "print(\"Training dataset memory consumption: {:.1f}MB\".format(train_X.nbytes / (1024 * 1024.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b437dcf4",
   "metadata": {},
   "source": [
    "<h1>k-NN Gesture Classification</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195314ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05968405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the k-NN classifier\n",
    "k = 3\n",
    "knn = KNeighborsClassifier(n_neighbors=k,\n",
    "                           n_jobs=-1)\n",
    "knn.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c91fd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the classifier on the test dataset\n",
    "knn_predictions = knn.predict(test_X)\n",
    "\n",
    "print(classification_report(test_y,\n",
    "                            knn_predictions,\n",
    "                            labels=gestures,\n",
    "                            target_names=gestures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e6677f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(knn_predictions, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860ea982",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb794d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(test_y, return_counts=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
